{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "batch_size = 32\n",
    "class_map = {\n",
    "    0:'airplane',\n",
    "    1:'automobile',\n",
    "    2:'bird',\n",
    "    3:'cat',\n",
    "    4:'deer',\n",
    "    5:'dog',\n",
    "    6:'frog',\n",
    "    7:'horse',\n",
    "    8:'ship',\n",
    "    9:'truck'\n",
    "}\n",
    "show_summary = True\n",
    "show_dataset_analyze = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Network and Print Summary\n",
    "from DeepLib.model import CifarNetDilated\n",
    "from DeepLib.utils import get_device, print_summary\n",
    "\n",
    "# Check GPU availability\n",
    "use_cuda, device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from DeepLib.dataset import get_loader\n",
    "from DeepLib.transform import get_a_train_transform, get_a_test_transform\n",
    "\n",
    "train_loader, test_loader = get_loader('CIFAR10',get_a_train_transform(), get_a_test_transform(), batch_size=batch_size, use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate Class Scale\n",
    "# from DeepLib.visualize import print_class_scale, print_samples\n",
    "# if show_dataset_analyze:\n",
    "#     # print_class_scale(train_loader, class_map)\n",
    "#     print_samples(train_loader,class_map)# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Epochs: 500\n",
      "Lr: 0.01\n",
      "Max Lr: 0.1\n",
      "Batch Size: 32\n",
      "Dropout: False\n",
      "\n",
      "\n",
      "Shape: torch.Size([2, 56, 8, 8])\n",
      "Shape: torch.Size([2, 56, 1, 1])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 32, 32]             648\n",
      "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
      "         Dropout2d-3           [-1, 24, 32, 32]               0\n",
      "            Conv2d-4           [-1, 32, 30, 30]           6,912\n",
      "       BatchNorm2d-5           [-1, 32, 30, 30]              64\n",
      "         Dropout2d-6           [-1, 32, 30, 30]               0\n",
      "            Conv2d-7           [-1, 32, 26, 26]           9,216\n",
      "       BatchNorm2d-8           [-1, 32, 26, 26]              64\n",
      "         Dropout2d-9           [-1, 32, 26, 26]               0\n",
      "           Conv2d-10           [-1, 40, 26, 26]          11,520\n",
      "      BatchNorm2d-11           [-1, 40, 26, 26]              80\n",
      "        Dropout2d-12           [-1, 40, 26, 26]               0\n",
      "           Conv2d-13           [-1, 40, 24, 24]          14,400\n",
      "      BatchNorm2d-14           [-1, 40, 24, 24]              80\n",
      "        Dropout2d-15           [-1, 40, 24, 24]               0\n",
      "           Conv2d-16           [-1, 40, 20, 20]          14,400\n",
      "      BatchNorm2d-17           [-1, 40, 20, 20]              80\n",
      "        Dropout2d-18           [-1, 40, 20, 20]               0\n",
      "           Conv2d-19           [-1, 48, 20, 20]          17,280\n",
      "      BatchNorm2d-20           [-1, 48, 20, 20]              96\n",
      "        Dropout2d-21           [-1, 48, 20, 20]               0\n",
      "           Conv2d-22           [-1, 48, 18, 18]          20,736\n",
      "      BatchNorm2d-23           [-1, 48, 18, 18]              96\n",
      "        Dropout2d-24           [-1, 48, 18, 18]               0\n",
      "           Conv2d-25           [-1, 48, 14, 14]          20,736\n",
      "      BatchNorm2d-26           [-1, 48, 14, 14]              96\n",
      "        Dropout2d-27           [-1, 48, 14, 14]               0\n",
      "           Conv2d-28           [-1, 56, 14, 14]          24,192\n",
      "      BatchNorm2d-29           [-1, 56, 14, 14]             112\n",
      "        Dropout2d-30           [-1, 56, 14, 14]               0\n",
      "           Conv2d-31           [-1, 56, 12, 12]          28,224\n",
      "      BatchNorm2d-32           [-1, 56, 12, 12]             112\n",
      "        Dropout2d-33           [-1, 56, 12, 12]               0\n",
      "           Conv2d-34             [-1, 56, 8, 8]          28,224\n",
      "      BatchNorm2d-35             [-1, 56, 8, 8]             112\n",
      "        Dropout2d-36             [-1, 56, 8, 8]               0\n",
      "        AvgPool2d-37             [-1, 56, 1, 1]               0\n",
      "           Linear-38                   [-1, 10]           1,130\n",
      "================================================================\n",
      "Total params: 198,658\n",
      "Trainable params: 198,658\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 4.76\n",
      "Params size (MB): 0.76\n",
      "Estimated Total Size (MB): 5.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "from DeepLib.backpropagation import train, test, get_sgd_optimizer\n",
    "from DeepLib.utils import initialize_weights\n",
    "from DeepLib.scheduler import one_cycle_lr_pt, one_cycle_lr_custom\n",
    "from DeepLib.training import Training\n",
    "\n",
    "# Train Params\n",
    "epochs = 500\n",
    "lr = 0.01\n",
    "max_lr = 0.1\n",
    "steps_per_epoch = len(train_loader)\n",
    "dropout = False\n",
    "momentum = 0.9\n",
    "weight_decay = 0.000125\n",
    "\n",
    "print(\"Using Device:\", device)\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Lr:\", lr)\n",
    "print(\"Max Lr:\", max_lr)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Dropout:\", dropout)\n",
    "print(\"\\n\")\n",
    "\n",
    "# bnmodel = CifarNet(norm='bn',base_channels=12).apply(initialize_weights).to(device)\n",
    "model = CifarNetDilated().apply(initialize_weights).to(device)\n",
    "print_summary(model, input_size=(3, 32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch | LR       | Time    | TrainLoss | TrainCorrect | TrainAcc | ValLoss  | ValCorrect | ValAcc |\n",
      "Shape: torch.Size([32, 56, 8, 8])\n",
      "Shape: torch.Size([32, 56, 1, 1])\n",
      "DataShape: torch.Size([32, 3, 32, 32])\n",
      "Target Shape: torch.Size([32])\n",
      "Output Shape: torch.Size([16, 10])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (16) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 37\u001b[0m\n\u001b[0;32m     14\u001b[0m custom_scheduler \u001b[39m=\u001b[39m one_cycle_lr_custom(\n\u001b[0;32m     15\u001b[0m     optimizer, \n\u001b[0;32m     16\u001b[0m     lr\u001b[39m=\u001b[39mlr, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     anneal_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m trainer \u001b[39m=\u001b[39m Training(\n\u001b[0;32m     24\u001b[0m     model,\n\u001b[0;32m     25\u001b[0m     optimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     dropout\n\u001b[0;32m     35\u001b[0m )\n\u001b[1;32m---> 37\u001b[0m trainer\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mDeepLib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvisualize\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_network_performance\n\u001b[0;32m     41\u001b[0m \u001b[39m# trainer.print_best_model()\u001b[39;00m\n",
      "File \u001b[1;32md:\\Github\\85-Cifar-10-using-depthwise-seperable-and-dilated-kernels\\DeepLib\\training.py:111\u001b[0m, in \u001b[0;36mTraining.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39m#             self.log_epoch_params(epoch)\u001b[39;00m\n\u001b[0;32m    109\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 111\u001b[0m             train_loss, train_correct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m    112\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscheduler)\n\u001b[0;32m    113\u001b[0m             valid_loss, valid_correct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest(\n\u001b[0;32m    114\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_loader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    116\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist_train_loss\u001b[39m.\u001b[39mappend(train_loss)\n",
      "File \u001b[1;32md:\\Github\\85-Cifar-10-using-depthwise-seperable-and-dilated-kernels\\DeepLib\\backpropagation.py:39\u001b[0m, in \u001b[0;36mtrain.<locals>.internal\u001b[1;34m(model, train_loader, optimizer, dropout, device, scheduler)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTarget Shape:\u001b[39m\u001b[39m\"\u001b[39m,target\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     38\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOutput Shape:\u001b[39m\u001b[39m\"\u001b[39m,output\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 39\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mnll_loss(output, target)\n\u001b[0;32m     40\u001b[0m \u001b[39mif\u001b[39;00m use_l1 \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     l1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nerfstudio\\lib\\site-packages\\torch\\nn\\functional.py:2704\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2703\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2704\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (16) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "optimizer = get_sgd_optimizer(model, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Pytorch one cycle scheduler\n",
    "pytorch_scheduler = one_cycle_lr_pt(\n",
    "    optimizer, \n",
    "    lr=lr, \n",
    "    max_lr=max_lr, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=epochs, \n",
    "    anneal_strategy='linear'\n",
    ")\n",
    "\n",
    "# One cycle schedule with a custom function\n",
    "custom_scheduler = one_cycle_lr_custom(\n",
    "    optimizer, \n",
    "    lr=lr, \n",
    "    max_lr=max_lr, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=epochs, \n",
    "    anneal_strategy='linear'\n",
    ")\n",
    "\n",
    "trainer = Training(\n",
    "    model,\n",
    "    optimizer,\n",
    "    custom_scheduler,\n",
    "    train(),\n",
    "    test,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    lr,\n",
    "    epochs,\n",
    "    device,\n",
    "    dropout\n",
    ")\n",
    "\n",
    "trainer.run()\n",
    "\n",
    "from DeepLib.visualize import plot_network_performance\n",
    "\n",
    "# trainer.print_best_model()\n",
    "plot_network_performance(epochs, trainer.schedule, trainer.list_train_loss, trainer.list_valid_loss, trainer.list_train_correct, trainer.list_valid_correct)\n",
    "\n",
    "from DeepLib.utils import get_all_predictions, get_incorrrect_predictions, prepare_confusion_matrix\n",
    "from DeepLib.visualize import plot_confusion_matrix\n",
    "\n",
    "all_preds, all_targets = get_all_predictions(model, test_loader, device)\n",
    "confusion_matrix = prepare_confusion_matrix(all_preds, all_targets, class_map)\n",
    "plot_confusion_matrix(confusion_matrix, class_map, False)\n",
    "\n",
    "\n",
    "from DeepLib.utils import get_incorrrect_predictions\n",
    "from DeepLib.visualize import plot_incorrect_predictions\n",
    "\n",
    "incorrect = get_incorrrect_predictions(model, test_loader, device)\n",
    "plot_incorrect_predictions(incorrect, class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerfstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
